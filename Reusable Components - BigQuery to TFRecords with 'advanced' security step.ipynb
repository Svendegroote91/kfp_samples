{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!PIP_DISABLE_PIP_VERSION_CHECK=1 pip3 install kfp --upgrade --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRSTNAME_LASTNAME = 'firstname_lastname'\n",
    "EXPERIMENT_NAME = 'Hello world! - BigQuery to TFrecords ' + FIRSTNAME_LASTNAME\n",
    "BUCKET_NAME = '<BUCKET_NAME>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'meetup-kfp'\n",
    "REGION = 'europe-west1'\n",
    "DATASET_SIZE = 10000\n",
    "GCS_WORKING_DIR = 'gs://{}'.format(BUCKET_NAME) # No ending slash\n",
    "GCS_OUTPUT_DIR = '{}/bigquery_to_tfrecords/output/{}'.format(GCS_WORKING_DIR, FIRSTNAME_LASTNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load component definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aihub.cloud.google.com/u/0/p/products%2F28a006d0-c833-4c68-98ff-37358eeb7726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import kfp.components as comp\n",
    "\n",
    "bigquery_tfrecords_op = comp.load_component_from_url('https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.5/kfp-components/data_converter/bigquery_to_tfrecords/component.yaml')\n",
    "help(bigquery_tfrecords_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define component definitions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### google cloud Auth methods\n",
    "To retrieve a GCP secret from K8 you can use the container operation \n",
    "```console\n",
    ".apply(use_gcp_secret('secret name'))\n",
    "```\n",
    "Application Default Credentials (ADC) provide a method to get credentials used in calling Google APIs.However to add the secret to user enviroment to use for applications such as Gsutil you need to run the following \n",
    "``` console\n",
    "gcloud auth activate-service-account --key-file /secret/gcp-credentials/user-gcp-sa.json'''\n",
    "```\n",
    "The gcloud auth application-default command group allows you to manage active credentials on your machine that are used for local application development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically note that you can view, or add secrets to your K8 cluster using:\n",
    "\n",
    "```console\n",
    "kubectl get secrets\n",
    "kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls(dir_path):\n",
    "    import subprocess\n",
    "    print(subprocess.check_call(['gcloud', 'auth', 'activate-service-account', '--key-file','/secret/gcp-credentials/user-gcp-sa.json']))\n",
    "    print(subprocess.check_output(['gsutil', 'ls', dir_path]).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image needs gsutil\n",
    "ls_op = comp.func_to_container_op(ls, base_image='gcr.io/google.com/cloudsdktool/cloud-sdk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the pipeline\n",
    "Pipeline function has to be decorated with the `@dsl.pipeline` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import json\n",
    "@dsl.pipeline(\n",
    "    name='Bigquery query pipeline',\n",
    "    description='Bigquery query pipeline'\n",
    ")\n",
    "def bigquery_tfrecords_ls(\n",
    "    input_dir,\n",
    "    output,\n",
    "    project,\n",
    "    region,\n",
    "    float32='',\n",
    "    categorical='',\n",
    "    mode='local',\n",
    "    limit=-1,\n",
    "    num_workers=2\n",
    "):\n",
    "    bigquery_tfrecords_task = bigquery_tfrecords_op(\n",
    "        input_dir,\n",
    "        output,\n",
    "        project,\n",
    "        region,\n",
    "        float32,\n",
    "        categorical,\n",
    "        mode,\n",
    "        limit,\n",
    "        num_workers\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    ls_task = ls_op(bigquery_tfrecords_task.outputs['output']).apply(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = bigquery_tfrecords_ls\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n",
    "import kfp.compiler as compiler\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the pipeline for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get or create an experiment and submit a pipeline run\n",
    "import kfp\n",
    "client = kfp.Client()\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the component files (query, preprocess_row.py, etc) are stored in bigquery_to_tfrecords/ folder in your bucket!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify pipeline argument values\n",
    "arguments = {\n",
    "    'input_dir': 'gs://{}/bigquery_to_tfrecords'.format(BUCKET_NAME),\n",
    "    'output': GCS_OUTPUT_DIR,\n",
    "    'limit': DATASET_SIZE,\n",
    "    'mode': 'local' # mode 'local' = DirectRunner, mode 'dataflow' = DataflowRunner\n",
    "    'project': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "}\n",
    "\n",
    "\n",
    "#Submit a pipeline run\n",
    "run_name = pipeline_func.__name__ + ' ' + FIRSTNAME_LASTNAME + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
